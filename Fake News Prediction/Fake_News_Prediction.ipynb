{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8k2iPdQuhctQ"
      },
      "source": [
        "# Fake News Article Detection using Natural Language Processing\n",
        "\n",
        "In this Colab notebook, we will build a machine learning model to classify news articles as either \"Real\" or \"Fake\" based on their content. To achieve this, we will perform a series of text preprocessing and feature engineering steps, followed by training a Logistic Regression model.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "https://www.kaggle.com/c/fake-news/data?select=train.csv\n",
        "\n",
        "We will use a dataset of news articles, loaded from a CSV file, which includes information such as the author, title, and content of each article. The goal is to create a model that can effectively distinguish between genuine news articles and fake or misleading ones.\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "1. **Combining Author and Title**: We will concatenate the \"author\" and \"title\" columns into a single \"content\" column. This helps capture more information for classification.\n",
        "\n",
        "2. **Text Preprocessing**: We'll clean the text data by converting it to lowercase, removing non-alphabetical characters, and applying word stemming using the Porter Stemmer. Additionally, common English stopwords will be removed to focus on meaningful words.\n",
        "\n",
        "## Feature Engineering\n",
        "\n",
        "To convert the text data into a format suitable for machine learning, we will use the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique. This process will transform the text into numerical features while considering the importance of words in the corpus.\n",
        "\n",
        "## Model Building\n",
        "\n",
        "We will utilize a Logistic Regression classifier to train our model. This algorithm is a popular choice for text classification tasks due to its simplicity and effectiveness.\n",
        "\n",
        "## Model Evaluation\n",
        "\n",
        "We will assess the performance of our model by calculating accuracy scores on both the training and testing datasets. Additionally, we will select a sample from the test data and make predictions on it to demonstrate how the model classifies news articles.\n",
        "\n",
        "Let's proceed with the code step by step to build and evaluate our news article classification model.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdSMyicOWv2"
      },
      "source": [
        "# Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8ReG4pGZhDA"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qkset9LyOaa2"
      },
      "source": [
        "# Reading, Importing and modifying Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcVQQ5-MaBq1"
      },
      "outputs": [],
      "source": [
        "# Load the news dataset from a CSV file\n",
        "news_dataset = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_JDYAllZm9v"
      },
      "outputs": [],
      "source": [
        "# Download NLTK stopwords data\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnNMpKxvZ-zF"
      },
      "outputs": [],
      "source": [
        "# printing the stopwords in English\n",
        "print(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNnXnA0faGXZ"
      },
      "outputs": [],
      "source": [
        "news_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVonDW-RaIY7"
      },
      "outputs": [],
      "source": [
        "# print the first 5 rows of the dataframe\n",
        "news_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj-mup_NaLEo"
      },
      "outputs": [],
      "source": [
        "# counting the number of missing values in the dataset\n",
        "news_dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twm2cbpAaO6K"
      },
      "outputs": [],
      "source": [
        "# Fill any missing values in the dataset with an empty string\n",
        "news_dataset = news_dataset.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoTle62GaRwS"
      },
      "outputs": [],
      "source": [
        "# Combine 'author' and 'title' columns into a single 'content' column\n",
        "news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3K6owdOaYu8"
      },
      "outputs": [],
      "source": [
        "print(news_dataset['content'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QJFAfJv2P5zp"
      },
      "source": [
        "# Preping the data for Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_2XqyBoqQOE0"
      },
      "source": [
        "Data Seprating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTUkR-6Iaamc"
      },
      "outputs": [],
      "source": [
        "# separating the data & label\n",
        "X = news_dataset.drop(columns='label', axis=1)\n",
        "Y = news_dataset['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXSf2OIHacQd"
      },
      "outputs": [],
      "source": [
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YRgkbBS8QQlV"
      },
      "source": [
        "Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEqhoT77aivx"
      },
      "outputs": [],
      "source": [
        "# Initialize a Porter Stemmer for word stemming\n",
        "port_stem = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9nK6F-eallH"
      },
      "outputs": [],
      "source": [
        "# Define a function for text preprocessing (stemming, lowercasing, removing stopwords)\n",
        "def stemming(content):\n",
        "    # Remove non-alphabetical characters and replace them with spaces\n",
        "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
        "    # Convert the text to lowercase\n",
        "    stemmed_content = stemmed_content.lower()\n",
        "    # Split the text into individual words\n",
        "    stemmed_content = stemmed_content.split()\n",
        "    # Apply stemming using Porter Stemmer and remove stopwords\n",
        "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "    # Join the processed words back into a single string\n",
        "    stemmed_content = ' '.join(stemmed_content)\n",
        "    return stemmed_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpupFK4Fawb6"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing function to the 'content' column of the dataset\n",
        "news_dataset['content'] = news_dataset['content'].apply(stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5XsghYbazzz"
      },
      "outputs": [],
      "source": [
        "print(news_dataset['content'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wS6yXlN4QyZF"
      },
      "source": [
        "# Preprocessed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OfQCwrza3tD"
      },
      "outputs": [],
      "source": [
        "# Extract the preprocessed text data and labels\n",
        "X = news_dataset['content'].values\n",
        "Y = news_dataset['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVTlvRcea66W"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT17WJlEa78d"
      },
      "outputs": [],
      "source": [
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4m-OYcMa-Wm"
      },
      "outputs": [],
      "source": [
        "Y.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ddwdaXAfREVh"
      },
      "source": [
        "Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImSXeEbUbC1-"
      },
      "outputs": [],
      "source": [
        "# Initialize a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the preprocessed text data\n",
        "vectorizer.fit(X)\n",
        "\n",
        "# Transform the text data into TF-IDF vectors\n",
        "X = vectorizer.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dlDsk50bGl4"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OVtcQ8NJRW_O"
      },
      "source": [
        "Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCWyVlbbbGkw"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eP1OVbz7ReMO"
      },
      "source": [
        "# Model Initiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3Hw-FE4bK4I"
      },
      "outputs": [],
      "source": [
        "# Initialize a Logistic Regression model\n",
        "model = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOiV9iKmbNHg"
      },
      "outputs": [],
      "source": [
        "# Train the Logistic Regression model on the training data\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx9O-WmFbR76"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the training data and calculate training accuracy\n",
        "X_train_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ncxYGrVbWu7"
      },
      "outputs": [],
      "source": [
        "# Predict labels for the test data and calculate test accuracy\n",
        "X_test_prediction = model.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQE3Zp4mbcx9"
      },
      "outputs": [],
      "source": [
        "# Print the training and test accuracy scores\n",
        "print('Accuracy score of the training data:', training_data_accuracy)\n",
        "print('Accuracy score of the test data:', test_data_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7DSx86MARltQ"
      },
      "source": [
        "# Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwovNjXgblIv"
      },
      "outputs": [],
      "source": [
        "# Select a new sample from the test data for prediction\n",
        "X_new = X_test[3]\n",
        "\n",
        "# Use the trained model to predict the label of the new sample\n",
        "prediction = model.predict(X_new)\n",
        "\n",
        "# Print whether the news is classified as \"Real\" or \"Fake\" based on the prediction\n",
        "if prediction[0] == 0:\n",
        "    print('The news is Real')\n",
        "else:\n",
        "    print('The news is Fake')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f60f6MUeZYkD"
      },
      "outputs": [],
      "source": [
        "# Print the actual label of the selected sample\n",
        "print('Actual label:', Y_test[3])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
