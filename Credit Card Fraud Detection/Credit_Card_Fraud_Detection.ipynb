{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgt5FV6-Kwi"
      },
      "source": [
        "# **Credit Card Fraud Detection using Logistic Regression**\n",
        "\n",
        "In this Colab notebook, we'll explore a Credit Card Fraud Detection problem using a Logistic Regression model. Credit card fraud detection is a critical task in the financial industry, where the goal is to identify fraudulent transactions among a large number of legitimate ones.\n",
        "\n",
        "We will perform the following steps in this notebook:\n",
        "\n",
        "1. **Data Loading and Exploration:** We'll begin by importing necessary libraries, loading the credit card transaction dataset, and exploring the dataset to understand its structure and characteristics.\n",
        "\n",
        "2. **Data Preprocessing:** We'll handle missing values, examine the distribution of legitimate and fraudulent transactions, and perform under-sampling to create a balanced dataset for modeling.\n",
        "\n",
        "3. **Feature Engineering:** We'll split the data into features (X) and the target variable (Y).\n",
        "\n",
        "4. **Data Splitting:** We'll split the dataset into training and testing sets to evaluate our model's performance.\n",
        "\n",
        "5. **Model Training:** We'll create a Logistic Regression model and train it using the training data.\n",
        "\n",
        "6. **Model Evaluation:** We'll evaluate the model's performance using accuracy scores on both the training and testing datasets.\n",
        "\n",
        "## Dataset\n",
        "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
        "\n",
        "This notebook provides a step-by-step guide to building a fraud detection model and evaluating its effectiveness. Let's get started!\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2Lvf_ElITF"
      },
      "source": [
        "Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEShbCnXeB78"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uppOzPPBlScO"
      },
      "source": [
        "Reading, Importing and Modifying Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3owW86y_eDSU"
      },
      "outputs": [],
      "source": [
        "# Load the dataset into a Pandas DataFrame\n",
        "credit_card_data = pd.read_csv('/content/credit_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0a-mBEKeFJM"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the dataset\n",
        "credit_card_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UAFPJq0eKFt"
      },
      "outputs": [],
      "source": [
        "# Display the last 5 rows of the dataset\n",
        "credit_card_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5m54f3yeO1W"
      },
      "outputs": [],
      "source": [
        "# Provide information about the dataset, including data types and non-null counts\n",
        "credit_card_data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge6zlqCoeS0Z"
      },
      "outputs": [],
      "source": [
        "# Check the number of missing values in each column\n",
        "credit_card_data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inHJiJ97eYjo"
      },
      "outputs": [],
      "source": [
        "# Count the distribution of legitimate transactions (0) and fraudulent transactions (1)\n",
        "credit_card_data['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHJbOyJcelo8"
      },
      "outputs": [],
      "source": [
        "# Separate the data into legitimate and fraudulent transactions\n",
        "# 0 --> Normal Transaction\n",
        "# 1 --> Fraudulent Transaction\n",
        "legit = credit_card_data[credit_card_data.Class == 0]\n",
        "fraud = credit_card_data[credit_card_data.Class == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFdMr4f1ep1t"
      },
      "outputs": [],
      "source": [
        "# Display the shape (number of rows and columns) of the legitimate and fraudulent data\n",
        "print(legit.shape)\n",
        "print(fraud.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0PzpxreetYF"
      },
      "outputs": [],
      "source": [
        "# Calculate statistical measures for the amount in legitimate transactions\n",
        "legit.Amount.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulj9zat7euDu"
      },
      "outputs": [],
      "source": [
        "# Calculate statistical measures for the amount in fraudulent transactions\n",
        "fraud.Amount.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG7X-SqteyCu"
      },
      "outputs": [],
      "source": [
        "# Compare the mean values for both legitimate and fraudulent transactions\n",
        "credit_card_data.groupby('Class').mean()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Aylqqi1Aez0H"
      },
      "source": [
        "This dataset is highly unbalanced, as the number of Fraudulent Transactions --> 492\n",
        "\n",
        "Performing Under-Sampling i.e. Build a sample dataset containing a similar distribution of normal transactions and fraudulent transactions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4BxK2_Tn9wMK"
      },
      "source": [
        "# Model Initiation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pdOefEtPtGjw"
      },
      "source": [
        "### Preping the data for Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F0Lit9ve32p"
      },
      "outputs": [],
      "source": [
        "# Sample a subset of legitimate transactions with 492 entries\n",
        "legit_sample = legit.sample(n=492)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r0QX_9watL78"
      },
      "source": [
        "Concatenating two DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjlAWArve8y5"
      },
      "outputs": [],
      "source": [
        "# Concatenate two DataFrames to create a new dataset\n",
        "new_dataset = pd.concat([legit_sample, fraud], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaaXs0r5fF31"
      },
      "outputs": [],
      "source": [
        "# Display the first 5 rows of the new dataset\n",
        "new_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h435IdqfJYl"
      },
      "outputs": [],
      "source": [
        "# Display the last 5 rows of the new dataset\n",
        "new_dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM6YjgwJfMN9"
      },
      "outputs": [],
      "source": [
        "# Count the distribution of legitimate (0) and fraudulent (1) transactions in the new dataset\n",
        "new_dataset['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s6tB3pQfQlW"
      },
      "outputs": [],
      "source": [
        "# Calculate and display the mean values for both classes in the new dataset\n",
        "new_dataset.groupby('Class').mean()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2XIsUwldfTLH"
      },
      "source": [
        "Splitting the data into Features (X) and Targets (Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaJA72N5fW4J"
      },
      "outputs": [],
      "source": [
        "# Separate the data into features (X) and the target variable (Y)\n",
        "X = new_dataset.drop(columns='Class', axis=1)\n",
        "Y = new_dataset['Class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NvK9FRRfaVh"
      },
      "outputs": [],
      "source": [
        "# Print the features (X) and the target variable (Y)\n",
        "print(X)\n",
        "print(Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6CfZRI4Gfgkb"
      },
      "source": [
        "Split the data into Training Data and Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX_hGa7SfhqS"
      },
      "outputs": [],
      "source": [
        "# Split the data into training data and testing data with a 80/20 split ratio\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZ_znBC8flqU"
      },
      "outputs": [],
      "source": [
        "# Display the shapes of the datasets\n",
        "print(X.shape, X_train.shape, X_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lGUjWep6fpMz"
      },
      "source": [
        "Model Training using Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhXX-X66fqVU"
      },
      "outputs": [],
      "source": [
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpNiolHDfz1e"
      },
      "outputs": [],
      "source": [
        "# Train the Logistic Regression model using the training data\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gqFlcZADf1i3"
      },
      "source": [
        "Model Evaluation using Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6moYAJIHf2z_"
      },
      "outputs": [],
      "source": [
        "# Calculate and display the accuracy on the training data\n",
        "X_train_prediction = model.predict(X_train)\n",
        "training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n",
        "print('Accuracy on Training data: ', training_data_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCw6euCvdR-w"
      },
      "outputs": [],
      "source": [
        "# Calculate and display the accuracy on the test data\n",
        "X_test_prediction = model.predict(X_test)\n",
        "test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n",
        "print('Accuracy score on Test Data: ', test_data_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
